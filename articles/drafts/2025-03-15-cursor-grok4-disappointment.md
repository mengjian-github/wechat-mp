# Cursor+Grok4实测：跑分第一，体验垫底！

大家好，我是孟健。

最近，AI圈又炸了。

马斯克的xAI团队刚刚官宣了Grok 4的发布，还没正式上线，跑分就已经"逆天"——在号称"人类最后考试"（HLE）上，Grok 4最高拿下了45%的全场第一，直接把谷歌Gemini 2.5 Pro、OpenAI o3和Claude 4 Opus甩在身后。

看到这个消息，我的第一反应是：卧槽，这是要统一江湖的节奏？

Cursor最新也支持了Grok4的调用，于是我赶紧在Cursor当中实测了一下。

结果却让我大跌眼镜...

**从跑分来看，Grok 4确实很强，但实际体验却让人抓狂。**

## 纸面数据：Grok 4的"王者"表现

先说说让所有人都震惊的跑分数据。

根据基准测试，Grok 4在多个权威榜单上全面领先：

- **GPQA（研究生级物理与天文学）**：Grok 4得分87-88%，超越Gemini 2.5 Pro和Claude 4 Opus
- **AIME 25（美国数学邀请赛）**：Grok 4高达95%，远超Claude 4 Opus的75.5%
- **SWE-bench（真实软件工程问题）**：Grok 4 Code得分72-75%，同样领先OpenAI和Anthropic

最炸裂的是**HLE（Human Last Exam）**——这个跨100+学科、2500道专家级题目的终极闭卷考试，Grok 4最高分45%，是Gemini 2.5 Pro的两倍，Claude 4 Opus的四倍。

看到这些数据，我内心是激动的。毕竟作为一个天天和AI打交道的人，谁不希望有更强的工具来提升工作效率呢？

但是，理想很丰满，现实很骨感。

## 实测体验：又慢又懒的"智障"

我满怀期待地打开Cursor，切换到Grok 4，并且开启了Max模式，准备体验一下这个"跑分王者"的实力。

### 第一个坑：基础命令都能搞错

我先让它帮我处理一些日常的编程任务。

结果第一个问题就来了：**它执行命令的时候，会把 & 符号给转义了**，导致命令无法执行。

这是一个非常低级的错误，但Grok 4就是一直在那里兜圈子，死活不承认自己的问题。

我当时就懵了：这就是跑分第一的模型？连基础的命令都搞不定？

### 第二个坑：思考不透明，执行超级慢

接着，我让它做一个稍微复杂点的操作——帮我画Figma设计稿。

这下更崩溃了。

**它的思考过程完全不透明**，你根本不知道它在想什么，只能看着那个加载图标转啊转。

而且**速度奇慢无比**，我都怀疑它是不是在后台挖矿。

最要命的是，等了半天，它画出来的结果简直让我怀疑人生：

- 页面设计丑得要命
- 布局混乱不堪
- 明显偷工减料，页面没画完就停止了

### 第三个坑：对比实验让人绝望

为了验证不是我的问题，我用同样的提示词测试了Claude 4 Opus。

结果差距简直天壤之别：

- Claude的设计稿布局合理，美观大方
- 执行速度快，思考过程清晰
- 完整度高，细节处理到位

这一对比，我只能说：**Grok 4，使用起来就是浪费生命！**

## 深度思考：跑分与实用性的巨大鸿沟

这次实测让我想起了之前o3 pro刚出来的时候。

当时o3 pro的跑分也特别高，但实际表现却非常一般。Claude 4的Opus用起来跟Sonnet并没有很大区别，Sonnet解决不了的问题，Opus大概率也解决不了。

**这暴露了一个严重的问题：我们该反思大模型的评测基准了。**

### 跑分与实用性为什么会脱节？

1. **测试环境vs实际环境**：实验室的标准化测试和真实世界的复杂场景完全不同
2. **单项能力vs综合体验**：跑分测试的是单一维度的能力，但用户需要的是综合体验
3. **理论性能vs工程实现**：API调用的稳定性、响应速度、错误处理等工程问题在跑分中体现不出来

### 推理模型的"虚假繁荣"

现在的推理模型看起来分数是越来越高了，数学解题能力越来越强，但实际上呢？

**它们在解决真实世界问题时，往往表现平平。**

这就像是应试教育培养出来的"高分低能"学生——考试成绩很好，但一到实际工作就抓瞎。

## 营销炒作 vs 用户体验

说句不客气的话，现在AI圈的营销炒作已经到了令人发指的地步。

每次有新模型发布，都是：
- 跑分史上最高
- 超越人类极限
- 改变世界格局

但用户真正关心的是什么？

**是能不能帮我把工作做好，是能不能提升我的效率，是能不能解决我的实际问题。**

而不是在某个学术测试上拿了多少分。

### 给营销号们的建议

希望网络上的营销号们，不要再无脑吹了。

声势浩大，搞得轰轰烈烈，但你们自己真的去体验过吗？

**真正的好产品，是经得起用户真实使用的检验的。**

而不是靠PPT和跑分数据来忽悠人。

## 我的判断：Grok 4还远未成熟

经过这次实测，我的结论很明确：

**在我这里，Grok 4绝对不是SOTA，它已经在我的候选名单之外了。**

不是因为我对马斯克有偏见，也不是因为我不看好xAI。

而是因为：
1. **基础功能不稳定**：连简单的命令执行都会出错
2. **用户体验极差**：慢、不透明、结果质量低
3. **实用性不足**：跑分再高，解决不了实际问题就是零

### 对未来的期待

当然，我也不是完全否定Grok 4的潜力。

技术发展需要时间，产品优化需要迭代。

**但请先把基础功能做好，再来谈什么"超越人类"。**

下个版本，GPT-5、Claude、Gemini，哪个将胜出？

我们拭目以待。

但至少现在，在实用性这个最重要的维度上，Grok 4还有很长的路要走。

## 写在最后

作为一个长期关注AI发展的从业者，我希望看到的是：

**真正能帮助用户解决问题的AI产品，而不是只会在跑分榜上刷数据的"考试机器"。**

技术的进步应该服务于人，而不是为了炫技而炫技。

希望xAI团队能够重视用户反馈，把更多精力放在提升实际使用体验上。

毕竟，用户的时间和耐心都是有限的。

**跑分可以造假，但用户体验骗不了人。**

---

*如果你也有类似的体验，欢迎在评论区分享。让我们一起推动AI产品向更实用的方向发展。*
